{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Mapping data from JSON file and stroing into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ColumnNames_datatype</th>\n",
       "      <th>ColumnNames_dest</th>\n",
       "      <th>ColumnNames_source</th>\n",
       "      <th>db</th>\n",
       "      <th>tableName</th>\n",
       "      <th>transfermation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int</td>\n",
       "      <td>dest_col1</td>\n",
       "      <td>col1</td>\n",
       "      <td>xxxxxxzzzz</td>\n",
       "      <td>abc_table</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>string</td>\n",
       "      <td>dest_col3</td>\n",
       "      <td>col2</td>\n",
       "      <td>xxxxxxzzzz</td>\n",
       "      <td>abc_table</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>double</td>\n",
       "      <td>dest_col2</td>\n",
       "      <td>col3</td>\n",
       "      <td>xxxxxxzzzz</td>\n",
       "      <td>abc_table</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ColumnNames_datatype ColumnNames_dest ColumnNames_source          db  \\\n",
       "0                  int        dest_col1               col1  xxxxxxzzzz   \n",
       "1               string        dest_col3               col2  xxxxxxzzzz   \n",
       "2               double        dest_col2               col3  xxxxxxzzzz   \n",
       "\n",
       "   tableName transfermation  \n",
       "0  abc_table         update  \n",
       "1  abc_table         update  \n",
       "2  abc_table         update  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize    \n",
    "\n",
    "with open('emp_1.json') as data_file:    \n",
    "    data = json.load(data_file)  \n",
    "\n",
    "mapping_data = json_normalize(data, 'ColumnNames', ['db', 'tableName', 'transfermation'],record_prefix='ColumnNames_')\n",
    "\n",
    "mapping_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ColumnNames_datatype</th>\n",
       "      <th>ColumnNames_dest</th>\n",
       "      <th>ColumnNames_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int</td>\n",
       "      <td>dest_col1</td>\n",
       "      <td>col1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>string</td>\n",
       "      <td>dest_col3</td>\n",
       "      <td>col2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>double</td>\n",
       "      <td>dest_col2</td>\n",
       "      <td>col3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ColumnNames_datatype ColumnNames_dest ColumnNames_source\n",
       "0                  int        dest_col1               col1\n",
       "1               string        dest_col3               col2\n",
       "2               double        dest_col2               col3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_data[['ColumnNames_datatype','ColumnNames_dest','ColumnNames_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_df=spark.read.csv(\"file:///home/hduser/Desktop/Mandip/data.csv\",header=True,inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_df.write.parquet(\"file:///home/hduser/Desktop/Mandip/data_par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df=spark.read.parquet(\"file:///home/hduser/Desktop/Mandip/data_par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema of input parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: integer (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      " |-- col3: double (nullable = true)\n",
      " |-- col4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "par_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in input parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+-------+\n",
      "|col1|   col2| col3|   col4|\n",
      "+----+-------+-----+-------+\n",
      "|   1| value1|100.5|  data1|\n",
      "|   2| value2|105.5| data10|\n",
      "|   3| value3|110.5|data100|\n",
      "|   4| value4|115.5|  data2|\n",
      "|   5| value5|120.5| data11|\n",
      "|   6| value6|125.5|data101|\n",
      "|   7| value7|130.5|  data3|\n",
      "|   8| value8|135.5| data12|\n",
      "|   9| value9|140.5|data102|\n",
      "|  10|value10|145.5|  data4|\n",
      "|  11|value11|150.5| data13|\n",
      "|  12|value12|155.5|data103|\n",
      "|  13|value13|160.5|  data5|\n",
      "|  14|value14|165.5| data14|\n",
      "|  15|value15|170.5|data104|\n",
      "|  16|value16|175.5|  data6|\n",
      "|  17|value17|180.5| data15|\n",
      "|  18|value18|185.5|data105|\n",
      "|  19|value19|190.5|  data7|\n",
      "|  20|value20|195.5| data16|\n",
      "+----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "par_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.types import StringType,IntegerType,DoubleType,StructField,StructType\n",
    "# field =[]\n",
    "# col_names_dest=list(mapping_data['ColumnNames_dest'])\n",
    "# col_names_source=list(mapping_data['ColumnNames_source'])\n",
    "# data_types=list(mapping_data['ColumnNames_datatype'])\n",
    "# for col_name,data_type in zip(col_names_dest,data_types):\n",
    "\n",
    "#     if data_type==\"string\":\n",
    "#         field.append(StructField(col_name,StringType()))\n",
    "#     elif data_type==\"int\":\n",
    "#         field.append(StructField(col_name,IntegerType()))\n",
    "#     elif data_type==\"double\":\n",
    "#         field.append(StructField(col_name,DoubleType()))\n",
    "#     else:\n",
    "#         print(\"Something went wrong with Data types\")\n",
    "        \n",
    "\n",
    "# schema = StructType(field)\n",
    "# dest_df= spark.createDataFrame(sc.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating temp View from input parquet file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df.createOrReplaceTempView(\"par_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query  formation to access data according to destination table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_dest=list(mapping_data['ColumnNames_dest'])\n",
    "col_names_source=list(mapping_data['ColumnNames_source'])\n",
    "data_types=list(mapping_data['ColumnNames_datatype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select col1 as dest_col1,col2 as dest_col3,col3 as dest_col2 from par_view'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryformation=\"\"\n",
    "for col_name_dest,col_name_source in zip(col_names_dest,col_names_source):\n",
    "    queryformation=queryformation+col_name_source+\" as \"+col_name_dest+\",\"\n",
    "    \n",
    "queryformation=\"select \"+queryformation[0:-1]+\" from par_view\"\n",
    "queryformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_df=spark.sql(queryformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+\n",
      "|dest_col1|dest_col3|dest_col2|\n",
      "+---------+---------+---------+\n",
      "|        1|   value1|    100.5|\n",
      "|        2|   value2|    105.5|\n",
      "|        3|   value3|    110.5|\n",
      "|        4|   value4|    115.5|\n",
      "|        5|   value5|    120.5|\n",
      "|        6|   value6|    125.5|\n",
      "|        7|   value7|    130.5|\n",
      "|        8|   value8|    135.5|\n",
      "|        9|   value9|    140.5|\n",
      "|       10|  value10|    145.5|\n",
      "|       11|  value11|    150.5|\n",
      "|       12|  value12|    155.5|\n",
      "|       13|  value13|    160.5|\n",
      "|       14|  value14|    165.5|\n",
      "|       15|  value15|    170.5|\n",
      "|       16|  value16|    175.5|\n",
      "|       17|  value17|    180.5|\n",
      "|       18|  value18|    185.5|\n",
      "|       19|  value19|    190.5|\n",
      "|       20|  value20|    195.5|\n",
      "+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dest_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dest_col1: integer (nullable = true)\n",
      " |-- dest_col3: string (nullable = true)\n",
      " |-- dest_col2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dest_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type casting of columns before strong as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType,IntegerType,DoubleType,StructField,StructType\n",
    "spark_datatypes=[]\n",
    "for col_name,data_type in zip(col_names_dest,data_types):\n",
    "    if data_type==\"string\":\n",
    "        spark_datatypes.append(\"string\")\n",
    "    elif data_type==\"int\":\n",
    "        spark_datatypes.append(\"integer\")\n",
    "    elif data_type==\"double\":\n",
    "        spark_datatypes.append(\"double\")\n",
    "    else:\n",
    "        print(\"Something went wrong with Data types\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "for col_name_dest,spark_datatype in zip(col_names_dest,spark_datatypes):\n",
    "    finaldf = dest_df.withColumn(col_name_dest, col(col_name_dest).cast(spark_datatype))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dest_col1: integer (nullable = true)\n",
      " |-- dest_col3: string (nullable = true)\n",
      " |-- dest_col2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+\n",
      "|dest_col1|dest_col3|dest_col2|\n",
      "+---------+---------+---------+\n",
      "|        1|   value1|    100.5|\n",
      "|        2|   value2|    105.5|\n",
      "|        3|   value3|    110.5|\n",
      "|        4|   value4|    115.5|\n",
      "|        5|   value5|    120.5|\n",
      "|        6|   value6|    125.5|\n",
      "|        7|   value7|    130.5|\n",
      "|        8|   value8|    135.5|\n",
      "|        9|   value9|    140.5|\n",
      "|       10|  value10|    145.5|\n",
      "|       11|  value11|    150.5|\n",
      "|       12|  value12|    155.5|\n",
      "|       13|  value13|    160.5|\n",
      "|       14|  value14|    165.5|\n",
      "|       15|  value15|    170.5|\n",
      "|       16|  value16|    175.5|\n",
      "|       17|  value17|    180.5|\n",
      "|       18|  value18|    185.5|\n",
      "|       19|  value19|    190.5|\n",
      "|       20|  value20|    195.5|\n",
      "+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing dataframe as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.write.parquet(\"file:///home/hduser/Desktop/Mandip/data_par_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
